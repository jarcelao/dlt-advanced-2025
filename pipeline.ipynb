{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c1f5ab",
   "metadata": {},
   "source": [
    "# Lesson 9: Performance Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22797078",
   "metadata": {},
   "source": [
    "This notebook contains my deliverables for Lesson 9 of the dlt Advanced course.\n",
    "\n",
    "It contains a dlt pipeline that queries the below endpoints from the [Jaffle Shop API](https://jaffle-shop.scalevector.ai/docs)\n",
    "* `/customers`\n",
    "* `/orders`\n",
    "* `/products`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e20b02e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19ffe37",
   "metadata": {},
   "source": [
    "This notebook assumes you're running this from the [project repo](https://github.com/jarcelao/dlt-advanced-2025). In other words, it assumes you have `uv` and the relevant project dependencies installed.\n",
    "\n",
    "Otherwise, you can run the below cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11fec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install dlt[duckdb]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0706f15b",
   "metadata": {},
   "source": [
    "## Naive Pipeline\n",
    "\n",
    "Let's start by implementing this the naive way. In this version, we follow dlt best practices, but keep the defaults for performance tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd702dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run started at 2025-05-10 05:08:16.911087+00:00 and COMPLETED in 16 minutes and 52.11 seconds with 4 steps.\n",
      "Step extract COMPLETED in 16 minutes and 25.26 seconds.\n",
      "\n",
      "Load package 1746853696.9559867 is EXTRACTED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "\n",
      "Step normalize COMPLETED in 15.00 seconds.\n",
      "Normalized data for the following tables:\n",
      "- orders: 61948 row(s)\n",
      "- orders__items: 90900 row(s)\n",
      "- customers: 935 row(s)\n",
      "- _dlt_pipeline_state: 1 row(s)\n",
      "- products: 10 row(s)\n",
      "\n",
      "Load package 1746853696.9559867 is NORMALIZED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "\n",
      "Step load COMPLETED in 11.82 seconds.\n",
      "Pipeline jaffle_shop_pipeline load step completed in 11.78 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset jaffle_shop_20250510050816\n",
      "The duckdb destination used duckdb:////workspaces/dlt-advanced-2025/jaffle_shop_pipeline.duckdb location to store data\n",
      "Load package 1746853696.9559867 is LOADED and contains no failed jobs\n",
      "\n",
      "Step run COMPLETED in 16 minutes and 52.11 seconds.\n",
      "Pipeline jaffle_shop_pipeline load step completed in 11.78 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset jaffle_shop_20250510050816\n",
      "The duckdb destination used duckdb:////workspaces/dlt-advanced-2025/jaffle_shop_pipeline.duckdb location to store data\n",
      "Load package 1746853696.9559867 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator\n",
    "\n",
    "\n",
    "client = RESTClient(\n",
    "    base_url=\"https://jaffle-shop.scalevector.ai/api/v1\",\n",
    "    paginator=PageNumberPaginator(base_page=1, total_path=\"\"),\n",
    ")\n",
    "\n",
    "\n",
    "@dlt.resource(\n",
    "    table_name=\"customers\",\n",
    "    write_disposition=\"merge\",\n",
    "    primary_key=\"id\"\n",
    ")\n",
    "def get_customers():\n",
    "    yield from client.paginate(\"/customers\")\n",
    "\n",
    "\n",
    "@dlt.resource(\n",
    "    table_name=\"orders\", \n",
    "    write_disposition=\"merge\", \n",
    "    primary_key=\"id\"\n",
    ")\n",
    "def get_orders():\n",
    "    yield from client.paginate(\"/orders\")\n",
    "\n",
    "\n",
    "@dlt.resource(\n",
    "    table_name=\"products\",\n",
    "    write_disposition=\"merge\",\n",
    "    primary_key=\"sku\"\n",
    ")\n",
    "def get_products():\n",
    "    yield from client.paginate(\"/products\")\n",
    "\n",
    "\n",
    "@dlt.source\n",
    "def jaffle_shop_data():\n",
    "    return get_customers(), get_orders(), get_products()\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"jaffle_shop_pipeline\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"jaffle_shop\",\n",
    "    dev_mode=True\n",
    ")\n",
    "\n",
    "\n",
    "pipeline.run(jaffle_shop_data())\n",
    "print(pipeline.last_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc3e6a2",
   "metadata": {},
   "source": [
    "That took its sweet time! Can we do a lot better if we applied optimization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a6e6fd",
   "metadata": {},
   "source": [
    "## Optimized Pipeline\n",
    "\n",
    "Now we run the pipeline with all optimization techniques applied\n",
    "- Chunking\n",
    "- Parallelism\n",
    "- Buffer control\n",
    "- File rotation\n",
    "- Worker tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a5111e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run started at 2025-05-10 05:26:48.773800+00:00 and COMPLETED in 7 minutes and 35.19 seconds with 4 steps.\n",
      "Step extract COMPLETED in 7 minutes and 12.70 seconds.\n",
      "\n",
      "Load package 1746854808.8049765 is EXTRACTED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "\n",
      "Step normalize COMPLETED in 14.81 seconds.\n",
      "Normalized data for the following tables:\n",
      "- _dlt_pipeline_state: 1 row(s)\n",
      "- customers: 935 row(s)\n",
      "- products: 10 row(s)\n",
      "- orders: 61948 row(s)\n",
      "- orders__items: 90900 row(s)\n",
      "\n",
      "Load package 1746854808.8049765 is NORMALIZED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "\n",
      "Step load COMPLETED in 7.65 seconds.\n",
      "Pipeline jaffle_shop_pipeline load step completed in 7.62 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset jaffle_shop_20250510052648\n",
      "The duckdb destination used duckdb:////workspaces/dlt-advanced-2025/jaffle_shop_pipeline.duckdb location to store data\n",
      "Load package 1746854808.8049765 is LOADED and contains no failed jobs\n",
      "\n",
      "Step run COMPLETED in 7 minutes and 35.18 seconds.\n",
      "Pipeline jaffle_shop_pipeline load step completed in 7.62 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset jaffle_shop_20250510052648\n",
      "The duckdb destination used duckdb:////workspaces/dlt-advanced-2025/jaffle_shop_pipeline.duckdb location to store data\n",
      "Load package 1746854808.8049765 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "import os\n",
    "\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator\n",
    "\n",
    "\n",
    "os.environ[\"EXTRACT__WORKERS\"] = \"4\"\n",
    "os.environ[\"DATA_WRITER__BUFFER_MAX_ITEMS\"] = \"10000\"\n",
    "os.environ[\"EXTRACT__DATA_WRITER__FILE_MAX_ITEMS\"] = \"10000\"\n",
    "\n",
    "os.environ[\"NORMALIZE__WORKERS\"] = \"4\"\n",
    "os.environ[\"NORMALIZE__DATA_WRITER__BUFFER_MAX_ITEMS\"] = \"10000\"\n",
    "os.environ[\"NORMALIZE__DATA_WRITER__FILE_MAX_ITEMS\"] = \"10000\"\n",
    "\n",
    "os.environ[\"LOAD__WORKERS\"] = \"4\"\n",
    "\n",
    "\n",
    "client = RESTClient(\n",
    "    base_url=\"https://jaffle-shop.scalevector.ai/api/v1\",\n",
    "    paginator=PageNumberPaginator(base_page=1, total_path=\"\"),\n",
    ")\n",
    "\n",
    "\n",
    "@dlt.resource(\n",
    "    table_name=\"customers\",\n",
    "    write_disposition=\"merge\",\n",
    "    primary_key=\"id\",\n",
    "    parallelized=True,\n",
    ")\n",
    "def get_customers():  # noqa: F811\n",
    "    yield from client.paginate(\"/customers\")\n",
    "\n",
    "\n",
    "@dlt.resource(\n",
    "    table_name=\"orders\",\n",
    "    write_disposition=\"merge\",\n",
    "    primary_key=\"id\",\n",
    "    parallelized=True\n",
    ")\n",
    "def get_orders():  # noqa: F811\n",
    "    yield from client.paginate(\"/orders?page_size=1000\")\n",
    "\n",
    "\n",
    "@dlt.resource(\n",
    "    table_name=\"products\",\n",
    "    write_disposition=\"merge\",\n",
    "    primary_key=\"sku\",\n",
    "    parallelized=True,\n",
    ")\n",
    "def get_products():  # noqa: F811\n",
    "    yield from client.paginate(\"/products\")\n",
    "\n",
    "\n",
    "@dlt.source\n",
    "def jaffle_shop_data():\n",
    "    return get_customers(), get_orders(), get_products()\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"jaffle_shop_pipeline\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"jaffle_shop\",\n",
    "    dev_mode=True\n",
    ")\n",
    "\n",
    "\n",
    "pipeline.run(jaffle_shop_data())\n",
    "print(pipeline.last_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09554843",
   "metadata": {},
   "source": [
    "That looks much better! Let's discuss more in the next part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9d0a21",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, we were able to cut down the overall pipeline runtime from 16 minutes and 52.11 seconds down to 7 minutes and 35.18 seconds. That's an estimated 2X speedup!\n",
    "\n",
    "I believe what contributed to this speedup the most were the optimizations to the Extract step of the pipeline. In particular, the `/orders` endpoint took the longest to extract in both pipeline versions. Tuning the workers here meant that I could maximize dlt's parallelization capabilities. I also increased the `page_size` parameter to 1000 so as to reduce the amount of API calls I needed to make."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
